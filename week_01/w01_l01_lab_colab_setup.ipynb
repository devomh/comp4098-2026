{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8b095a",
   "metadata": {},
   "source": [
    "# Lab: The Micro-Lifecycle Simulation\n",
    "\n",
    "## 1. Prerequisites & Setup\n",
    "*   **Goal:** Configure Google Colab and run a complete data lifecycle (Capture -> Archive) in one script.\n",
    "*   **Tools:** Python, DuckDB.\n",
    "\n",
    "### Environment Setup (Colab)\n",
    "Run this block to install the necessary libraries. This is the standard block we will use for most Analytical labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DuckDB and JupySQL for SQL magic in Notebooks\n",
    "!pip install duckdb duckdb-engine jupysql pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed06baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Configure SqlMagic to use DuckDB\n",
    "%load_ext sql\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "%sql duckdb:///:memory:\n",
    "# %sql duckdb:///weather.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec065448",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Stage 1: Capture (Ingest)\n",
    "We will simulate \"Capturing\" data by reading a raw CSV file. This represents raw data arriving from an external source.\n",
    "\n",
    "*We will use the `weather_raw.csv` file provided in the course data folder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0403eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating 'Capture' - Reading raw text data\n",
    "# (In a real scenario, this might be an API call or a log file)\n",
    "# Upload weather_raw.csv to Colaboratory\n",
    "\n",
    "# We use DuckDB to read the CSV directly into a relation\n",
    "# This is fast and efficient\n",
    "%sql SELECT * FROM read_csv_auto('weather_raw.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163168dc",
   "metadata": {},
   "source": [
    "**Observation:** Notice the `NULL` value (NaN in Pandas) and the potential errors in the data? That is normal for the \"Capture\" stage.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Stage 2: Store (Persist)\n",
    "Now we \"Store\" this data into a structured table. In a full architecture, this would be PostgreSQL, but we will use a DuckDB table to simulate the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Create a table with defined types (Schema)\n",
    "CREATE TABLE weather_staging (\n",
    "    date DATE,\n",
    "    city VARCHAR,\n",
    "    temp_c DOUBLE,\n",
    "    humidity INTEGER,\n",
    "    status VARCHAR\n",
    ");\n",
    "\n",
    "-- Load the raw data into our table\n",
    "INSERT INTO weather_staging \n",
    "SELECT * FROM read_csv_auto('weather_raw.csv');\n",
    "\n",
    "-- Verify storage\n",
    "SELECT count(*) as total_rows FROM weather_staging;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807db196",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Note:** If this operation fails (e.g., data type mismatch, constraint violation), DuckDB will leave the transaction in an aborted state. You'll need to run `%sql ROLLBACK;` before trying again. If the table was created but the INSERT failed, you may also need to drop the table first: `%sql DROP TABLE IF EXISTS weather_staging;`\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Stage 3 & 4: Process & Analyze\n",
    "Now we clean the data (Process) and find insights (Analyze).\n",
    "We want to remove errors and calculate the average temperature.\n",
    "\n",
    "### The \"Naive\" Approach (Python Loop)\n",
    "*Inefficient for millions of rows, but easy to read.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46739aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data to Pandas\n",
    "df = %sql SELECT * FROM weather_staging\n",
    "df_clean = df[df['status'] == 'active'].copy()\n",
    "print(f\"Average Temp (Python): {df_clean['temp_c'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b802f32",
   "metadata": {},
   "source": [
    "### The \"Better\" Approach (SQL Pushdown)\n",
    "*We let the database engine do the work. This is faster and scales.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Filter (Process) and Aggregate (Analyze) in one go\n",
    "SELECT \n",
    "    city,\n",
    "    AVG(temp_c) as avg_temp,\n",
    "    MAX(humidity) as max_humid\n",
    "FROM weather_staging\n",
    "WHERE status = 'active'\n",
    "GROUP BY city\n",
    "ORDER BY avg_temp DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db539ae4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Stage 5: Archive\n",
    "Finally, we save our valuable insights to a format optimized for long-term storage and other data science tools: **Parquet**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d32826",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Export the cleaned data to a Parquet file\n",
    "COPY (\n",
    "    SELECT * FROM weather_staging WHERE status = 'active'\n",
    ") TO 'cleaned_weather.parquet' (FORMAT 'PARQUET');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308aa2f",
   "metadata": {},
   "source": [
    "**Verification:**\n",
    "You should see a `cleaned_weather.parquet` file appear in your file browser. This file is smaller and faster to read than the original CSV.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Your Turn! (Exercises)\n",
    "\n",
    "### Exercise 1: Ingest JSON\n",
    "**Task:** DuckDB can also read JSON. Try to read the provided `weather_raw.json` file.\n",
    "**Hint:** Look up `read_json_auto` in the DuckDB documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b510ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a query to read the 'weather_raw.json' file. Remember to first upload it to Colaboratory\n",
    "# %sql SELECT * FROM ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7133d",
   "metadata": {},
   "source": [
    "### Exercise 2: Add a Filter\n",
    "**Task:** Modify the Analytical Query in Stage 4 to only show cities with an average temperature > 5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- TODO: Write your SQL here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5c9e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Summary\n",
    "You just simulated the entire Data Engineering lifecycle in a few minutes!\n",
    "1.  **Captured** raw CSV.\n",
    "2.  **Stored** it in a Table.\n",
    "3.  **Processed** it by filtering 'active' status.\n",
    "4.  **Analyzed** it with Aggregations.\n",
    "5.  **Archived** the result to Parquet."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
