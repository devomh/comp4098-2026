{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb45f402",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Lab: Trade-offs and Star Schema Design\n",
    "\n",
    "## 1. Prerequisites & Setup\n",
    "*   **Context:** This lab explores the practical trade-offs between normalized and denormalized designs.\n",
    "*   **Goal:** Analyze query performance differences and design a star schema.\n",
    "*   **Concept Review:** Ensure you have read `w03_l06_concept_denormalization.md`.\n",
    "\n",
    "### Environment Setup\n",
    "Run this block first to set up the environment and create sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Run this cell first (required for Colab)\n",
    "# NOTE: Run cells in order. Variables from earlier sections are used later.\n",
    "!pip install -q pandas duckdb mermaid-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import time\n",
    "from mermaid import Mermaid\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Setup complete! Ready for trade-off analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0623e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## 2. The Scenario: E-Commerce Analytics\n",
    "You're a Data Scientist at an e-commerce company. The operational database is normalized (from Lesson 5). Now the business wants a dashboard showing:\n",
    "\n",
    "1.  Total revenue by product category\n",
    "2.  Top customers by purchase volume\n",
    "3.  Monthly sales trends\n",
    "\n",
    "Your job: decide whether to query the normalized tables directly or create a denormalized analytical layer.\n",
    "\n",
    "### Create the Normalized Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b23de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DuckDB in-memory database\n",
    "conn = duckdb.connect(':memory:')\n",
    "\n",
    "# Create normalized tables\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE customers (\n",
    "        customer_id INTEGER PRIMARY KEY,\n",
    "        name VARCHAR,\n",
    "        email VARCHAR,\n",
    "        city VARCHAR,\n",
    "        segment VARCHAR  -- 'Consumer', 'Corporate', 'Home Office'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE products (\n",
    "        product_id VARCHAR PRIMARY KEY,\n",
    "        name VARCHAR,\n",
    "        category VARCHAR,\n",
    "        unit_price DECIMAL(10,2)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE orders (\n",
    "        order_id INTEGER PRIMARY KEY,\n",
    "        order_date DATE,\n",
    "        customer_id INTEGER REFERENCES customers(customer_id)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE order_items (\n",
    "        order_id INTEGER REFERENCES orders(order_id),\n",
    "        product_id VARCHAR REFERENCES products(product_id),\n",
    "        quantity INTEGER,\n",
    "        PRIMARY KEY (order_id, product_id)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Normalized schema created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70110c4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Populate with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Insert customers\n",
    "customers = [\n",
    "    (i, f\"Customer_{i}\", f\"cust{i}@email.com\",\n",
    "     random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']),\n",
    "     random.choice(['Consumer', 'Corporate', 'Home Office']))\n",
    "    for i in range(1, 1001)\n",
    "]\n",
    "conn.executemany(\"INSERT INTO customers VALUES (?, ?, ?, ?, ?)\", customers)\n",
    "\n",
    "# Insert products\n",
    "categories = ['Electronics', 'Clothing', 'Home & Garden', 'Sports', 'Books']\n",
    "products = [\n",
    "    (f\"P{i:04d}\", f\"Product_{i}\", random.choice(categories), round(random.uniform(5, 500), 2))\n",
    "    for i in range(1, 201)\n",
    "]\n",
    "conn.executemany(\"INSERT INTO products VALUES (?, ?, ?, ?)\", products)\n",
    "\n",
    "# Insert orders and order_items\n",
    "start_date = date(2023, 1, 1)\n",
    "order_id = 1\n",
    "\n",
    "for _ in range(5000):  # 5000 orders\n",
    "    order_date = start_date + timedelta(days=random.randint(0, 729))  # 2 years of data\n",
    "    customer_id = random.randint(1, 1000)\n",
    "    conn.execute(\"INSERT INTO orders VALUES (?, ?, ?)\", (order_id, order_date, customer_id))\n",
    "\n",
    "    # Each order has 1-5 items\n",
    "    num_items = random.randint(1, 5)\n",
    "    selected_products = random.sample(range(1, 201), num_items)\n",
    "    for prod_num in selected_products:\n",
    "        product_id = f\"P{prod_num:04d}\"\n",
    "        quantity = random.randint(1, 10)\n",
    "        conn.execute(\"INSERT INTO order_items VALUES (?, ?, ?)\", (order_id, product_id, quantity))\n",
    "\n",
    "    order_id += 1\n",
    "\n",
    "print(f\"Inserted 1000 customers, 200 products, 5000 orders\")\n",
    "print(f\"Total order_items: {conn.execute('SELECT COUNT(*) FROM order_items').fetchone()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09c9e1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "Inserted 1000 customers, 200 products, 5000 orders\n",
    "Total order_items: ~15000 (varies due to random 1-5 items per order)\n",
    "~~~\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Step 1: Query the Normalized Schema\n",
    "\n",
    "Let's run the business queries on the normalized tables.\n",
    "\n",
    "> **Performance Note:** With only ~15K rows, both normalized and star schema queries will complete in milliseconds. DuckDB is highly optimized and the performance difference at this scale is minimal. In production systems with **millions of rows**, the star schema advantage becomes significant (10x+ speedup is common). We use this small dataset to demonstrate the *pattern*, not to prove performance claims.\n",
    "\n",
    "### Query 1: Revenue by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65845ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_normalized = \"\"\"\n",
    "SELECT\n",
    "    p.category,\n",
    "    SUM(oi.quantity * p.unit_price) AS total_revenue\n",
    "FROM order_items oi\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "GROUP BY p.category\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "result = conn.execute(query_normalized).df()\n",
    "normalized_time = time.time() - start\n",
    "\n",
    "print(f\"Query time (normalized): {normalized_time:.4f} seconds\")\n",
    "print(result)\n",
    "\n",
    "# Note: For more reliable timing with small queries, you could run multiple iterations:\n",
    "# times = [timeit.timeit(lambda: conn.execute(query_normalized).df(), number=1) for _ in range(100)]\n",
    "# print(f\"Average time: {sum(times)/len(times):.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c55973",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "Query time (normalized): ~0.01-0.05 seconds\n",
    "        category  total_revenue\n",
    "0    Electronics      XXXXXX.XX\n",
    "1       Clothing      XXXXXX.XX\n",
    "2  Home & Garden      XXXXXX.XX\n",
    "3         Sports      XXXXXX.XX\n",
    "4          Books      XXXXXX.XX\n",
    "~~~\n",
    "\n",
    "</details>\n",
    "\n",
    "### Query 2: Top 10 Customers by Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_top_customers = \"\"\"\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    c.name,\n",
    "    c.segment,\n",
    "    SUM(oi.quantity * p.unit_price) AS total_spent\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "GROUP BY c.customer_id, c.name, c.segment\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "result = conn.execute(query_top_customers).df()\n",
    "normalized_time_2 = time.time() - start\n",
    "\n",
    "print(f\"Query time (normalized): {normalized_time_2:.4f} seconds\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a870cc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "Query time (normalized): ~0.01-0.05 seconds\n",
    "   customer_id          name      segment  total_spent\n",
    "0          XXX  Customer_XXX  XXXXXXXXXX     XXXXX.XX\n",
    "1          XXX  Customer_XXX  XXXXXXXXXX     XXXXX.XX\n",
    "...\n",
    "~~~\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Step 2: Create a Denormalized Fact Table\n",
    "\n",
    "Now let's create a star schema with a pre-joined fact table.\n",
    "\n",
    "### Design the Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: dim_date will be added in Exercise 2\n",
    "Mermaid(\"\"\"\n",
    "erDiagram\n",
    "    fact_sales {\n",
    "        int sale_id PK\n",
    "        int order_id\n",
    "        int date_key FK\n",
    "        int customer_id FK\n",
    "        varchar product_id FK\n",
    "        int quantity\n",
    "        decimal unit_price\n",
    "        decimal line_total\n",
    "    }\n",
    "    dim_date {\n",
    "        int date_key PK\n",
    "        date full_date\n",
    "        int year\n",
    "        int month\n",
    "        varchar month_name\n",
    "        int quarter\n",
    "    }\n",
    "    dim_customer {\n",
    "        int customer_id PK\n",
    "        varchar name\n",
    "        varchar city\n",
    "        varchar segment\n",
    "    }\n",
    "    dim_product {\n",
    "        varchar product_id PK\n",
    "        varchar name\n",
    "        varchar category\n",
    "    }\n",
    "\n",
    "    fact_sales }|--|| dim_date : \"date_key\"\n",
    "    fact_sales }|--|| dim_customer : \"customer_id\"\n",
    "    fact_sales }|--|| dim_product : \"product_id\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf43cd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Build the Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fact_sales with pre-computed values\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE fact_sales AS\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER () AS sale_id,\n",
    "        o.order_id,\n",
    "        o.order_date,\n",
    "        o.customer_id,\n",
    "        oi.product_id,\n",
    "        oi.quantity,\n",
    "        p.unit_price,\n",
    "        (oi.quantity * p.unit_price) AS line_total,\n",
    "        EXTRACT(YEAR FROM o.order_date) AS year,\n",
    "        EXTRACT(MONTH FROM o.order_date) AS month\n",
    "    FROM orders o\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    JOIN products p ON oi.product_id = p.product_id\n",
    "\"\"\")\n",
    "\n",
    "# Create dimension tables (simplified from normalized tables)\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE dim_customer AS\n",
    "    SELECT customer_id, name, city, segment\n",
    "    FROM customers\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE dim_product AS\n",
    "    SELECT product_id, name, category\n",
    "    FROM products\n",
    "\"\"\")\n",
    "\n",
    "print(\"Star schema created!\")\n",
    "print(f\"fact_sales rows: {conn.execute('SELECT COUNT(*) FROM fact_sales').fetchone()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b357d2a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "Star schema created!\n",
    "fact_sales rows: ~15000\n",
    "~~~\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Step 3: Compare Query Performance\n",
    "\n",
    "### Revenue by Category (Star Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_star = \"\"\"\n",
    "SELECT\n",
    "    dp.category,\n",
    "    SUM(fs.line_total) AS total_revenue\n",
    "FROM fact_sales fs\n",
    "JOIN dim_product dp ON fs.product_id = dp.product_id\n",
    "GROUP BY dp.category\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "result = conn.execute(query_star).df()\n",
    "star_time = time.time() - start\n",
    "\n",
    "print(f\"Query time (star schema): {star_time:.4f} seconds\")\n",
    "print(f\"Query time (normalized):  {normalized_time:.4f} seconds\")\n",
    "print(f\"Speedup: {normalized_time/star_time:.2f}x\" if star_time > 0 else \"N/A\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d0820",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "Query time (star schema): ~0.005-0.02 seconds\n",
    "Query time (normalized):  ~0.01-0.05 seconds\n",
    "Speedup: 1.5-3x\n",
    "~~~\n",
    "\n",
    "With larger datasets, the difference becomes more pronounced.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Top Customers (Star Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3568b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_star_customers = \"\"\"\n",
    "SELECT\n",
    "    dc.customer_id,\n",
    "    dc.name,\n",
    "    dc.segment,\n",
    "    SUM(fs.line_total) AS total_spent\n",
    "FROM fact_sales fs\n",
    "JOIN dim_customer dc ON fs.customer_id = dc.customer_id\n",
    "GROUP BY dc.customer_id, dc.name, dc.segment\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "result = conn.execute(query_star_customers).df()\n",
    "star_time_2 = time.time() - start\n",
    "\n",
    "print(f\"Query time (star schema): {star_time_2:.4f} seconds\")\n",
    "print(f\"Query time (normalized):  {normalized_time_2:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07046e57",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Step 4: Analyze the Trade-offs\n",
    "\n",
    "### Storage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare storage (approximate via row counts and columns)\n",
    "normalized_size = conn.execute(\"\"\"\n",
    "    SELECT\n",
    "        (SELECT COUNT(*) FROM customers) AS customers,\n",
    "        (SELECT COUNT(*) FROM products) AS products,\n",
    "        (SELECT COUNT(*) FROM orders) AS orders,\n",
    "        (SELECT COUNT(*) FROM order_items) AS order_items\n",
    "\"\"\").df()\n",
    "\n",
    "star_size = conn.execute(\"\"\"\n",
    "    SELECT\n",
    "        (SELECT COUNT(*) FROM dim_customer) AS dim_customer,\n",
    "        (SELECT COUNT(*) FROM dim_product) AS dim_product,\n",
    "        (SELECT COUNT(*) FROM fact_sales) AS fact_sales\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Normalized schema row counts:\")\n",
    "print(normalized_size)\n",
    "print(\"\\nStar schema row counts:\")\n",
    "print(star_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40d5c0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "Normalized schema row counts:\n",
    "   customers  products  orders  order_items\n",
    "0       1000       200    5000        ~15000\n",
    "\n",
    "Star schema row counts:\n",
    "   dim_customer  dim_product  fact_sales\n",
    "0          1000          200      ~15000\n",
    "~~~\n",
    "\n",
    "Row counts are similar, but `fact_sales` has more columns (redundant `unit_price`, pre-computed `line_total`, `year`, `month`).\n",
    "\n",
    "**Column comparison:**\n",
    "- Normalized: 4 tables Ã— ~4 columns each = ~16 columns total\n",
    "- Star: 3 tables, but `fact_sales` alone has 10 columns (including redundant data)\n",
    "\n",
    "The storage overhead is the *price* of denormalization. Worth it when queries are frequent and updates are rare.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Update Scenario Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cf593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Product P0001's price changes from $X to $Y\n",
    "# How many rows need updating?\n",
    "\n",
    "print(\"If Product P0001's price changes:\")\n",
    "print(f\"  Normalized: 1 row (products table)\")\n",
    "\n",
    "p0001_orders = conn.execute(\"\"\"\n",
    "    SELECT COUNT(*) FROM fact_sales WHERE product_id = 'P0001'\n",
    "\"\"\").fetchone()[0]\n",
    "print(f\"  Star Schema: {p0001_orders} rows (fact_sales) OR keep historical prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ad76d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Discussion</summary>\n",
    "\n",
    "This is the key trade-off:\n",
    "*   **Normalized:** One update, always current, no history\n",
    "*   **Star Schema:** Many updates needed, BUT often we *want* historical prices preserved\n",
    "\n",
    "In analytics, keeping the original transaction price is often correct business logic (\"What did we actually charge?\").\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Your Turn! (Exercises)\n",
    "\n",
    "### Exercise 1: Monthly Sales Trend Query\n",
    "**Task:** Write both normalized and star schema queries to get monthly revenue for 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef202b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalized version (join orders, order_items, products)\n",
    "query_monthly_normalized = \"\"\"\n",
    "-- Your query here\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Star schema version (just fact_sales with year/month columns)\n",
    "query_monthly_star = \"\"\"\n",
    "-- Your query here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3735e21",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~python\n",
    "# Normalized\n",
    "query_monthly_normalized = \"\"\"\n",
    "SELECT\n",
    "    EXTRACT(YEAR FROM o.order_date) AS year,\n",
    "    EXTRACT(MONTH FROM o.order_date) AS month,\n",
    "    SUM(oi.quantity * p.unit_price) AS revenue\n",
    "FROM orders o\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE EXTRACT(YEAR FROM o.order_date) = 2024\n",
    "GROUP BY 1, 2\n",
    "ORDER BY 1, 2\n",
    "\"\"\"\n",
    "\n",
    "# Star Schema\n",
    "query_monthly_star = \"\"\"\n",
    "SELECT\n",
    "    year,\n",
    "    month,\n",
    "    SUM(line_total) AS revenue\n",
    "FROM fact_sales\n",
    "WHERE year = 2024\n",
    "GROUP BY year, month\n",
    "ORDER BY year, month\n",
    "\"\"\"\n",
    "~~~\n",
    "\n",
    "The star schema query is simpler and doesn't require extracting date parts on the fly.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Exercise 2: Add a Time Dimension\n",
    "\n",
    "> **ðŸ’¡ Why Create a Separate `dim_date` Table?**\n",
    ">\n",
    "> **Current approach:** We extracted `year` and `month` directly in the fact table (lines 295-296). This works for simple queries.\n",
    ">\n",
    "> **Production pattern:** Create a separate `dim_date` table because:\n",
    "> 1. **Rich attributes**: Month names, quarters, weekends, holidays, fiscal periods\n",
    "> 2. **Pre-computed**: Calculate once, not on every query (faster for millions of rows)\n",
    "> 3. **Business logic**: Fiscal years, custom calendars, holiday rules\n",
    "> 4. **Consistency**: Same date logic across all fact tables in your warehouse\n",
    "> 5. **Flexibility**: Add new attributes (e.g., \"is_holiday\") without touching fact tables\n",
    ">\n",
    "> **Example benefit:** Instead of complex date math in every query, just:\n",
    "> ```sql\n",
    "> WHERE dd.is_weekend = FALSE AND dd.is_holiday = FALSE\n",
    "> ```\n",
    "\n",
    "**Task:** Build a `dim_date` dimension table following the industry-standard star schema pattern.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2a: Generate All Dates**\n",
    "\n",
    "> **Why Generate All Dates? Can't we just use dates from the `orders` table?**\n",
    ">\n",
    "> **Answer:** No! `dim_date` must contain **every date**, not just dates with orders. Here's why:\n",
    ">\n",
    "> **Problem with using only order dates:**\n",
    "> - **Missing dates:** If no orders on 2024-12-25, Christmas won't exist in `dim_date`\n",
    "> - **Broken charts:** Revenue reports will have gaps (can't show \"$0 on Dec 25\")\n",
    "> - **Can't analyze absences:** Can't find \"which weekends had zero sales?\" if those dates don't exist\n",
    "> - **Future planning:** Can't show next month's calendar if orders don't exist yet\n",
    ">\n",
    "> **Think of `dim_date` as a calendar on the wall** - it has every date, whether events happened or not.\n",
    ">\n",
    "> **Example benefit:**\n",
    "> ```sql\n",
    "> -- Show daily sales for January 2024 (including days with $0)\n",
    "> SELECT dd.full_date, COALESCE(SUM(fs.line_total), 0) as revenue\n",
    "> FROM dim_date dd\n",
    "> LEFT JOIN fact_sales fs ON dd.date_key = fs.date_key\n",
    "> WHERE dd.year = 2024 AND dd.month = 1\n",
    "> ```\n",
    "> This only works if dim_date has ALL days of January, not just days with orders!\n",
    "\n",
    "First, we need a table with one row per date. DuckDB provides a `range()` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a date series for 2023-2024\n",
    "# Hint: DuckDB's range(start_date, end_date, interval) generates a series\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE dim_date_basic AS\n",
    "    SELECT CAST(range AS DATE) AS full_date\n",
    "    FROM range(DATE '2023-01-01', DATE '2025-01-01', INTERVAL 1 DAY)\n",
    "\"\"\")\n",
    "\n",
    "# Check what we created\n",
    "print(\"Sample dates:\")\n",
    "print(conn.execute(\"SELECT * FROM dim_date_basic LIMIT 5\").df())\n",
    "print(f\"\\nTotal dates: {conn.execute('SELECT COUNT(*) FROM dim_date_basic').fetchone()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e72382",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "Sample dates:\n",
    "   full_date\n",
    "0 2023-01-01\n",
    "1 2023-01-02\n",
    "2 2023-01-03\n",
    "3 2023-01-04\n",
    "4 2023-01-05\n",
    "\n",
    "Total dates: 730 (2 years of dates)\n",
    "~~~\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2b: Extract Date Components**\n",
    "\n",
    "Now let's add useful date attributes. SQL provides two main functions for working with dates:\n",
    "\n",
    "| Function | Purpose | Example |\n",
    "|:---|:---|:---|\n",
    "| `EXTRACT(part FROM date)` | Get numeric part of date | `EXTRACT(YEAR FROM '2024-03-15')` â†’ 2024 |\n",
    "| `STRFTIME(date, format)` | Format date as string | `STRFTIME('2024-03-15', '%B')` â†’ 'March' |\n",
    "\n",
    "**Common EXTRACT parts:** `YEAR`, `MONTH`, `DAY`, `QUARTER`, `DAYOFWEEK` (0=Sunday, 6=Saturday)\n",
    "\n",
    "**Common STRFTIME formats:**\n",
    "- `'%Y'` = 4-digit year (2024)\n",
    "- `'%m'` = 2-digit month (03)\n",
    "- `'%B'` = Full month name (March)\n",
    "- `'%Y%m%d'` = YYYYMMDD format (20240315) - common for date keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18061b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add year, month, and month_name columns\n",
    "# Hint: Use EXTRACT for numbers, STRFTIME for text\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE dim_date_enriched AS\n",
    "    SELECT\n",
    "        full_date,\n",
    "        EXTRACT(YEAR FROM full_date) AS year,\n",
    "        EXTRACT(MONTH FROM full_date) AS month,\n",
    "        STRFTIME(full_date, '%B') AS month_name\n",
    "        -- We'll add more columns in the next step\n",
    "    FROM dim_date_basic\n",
    "\"\"\")\n",
    "\n",
    "print(conn.execute(\"SELECT * FROM dim_date_enriched LIMIT 5\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aeec3e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "   full_date  year  month month_name\n",
    "0 2023-01-01  2023      1    January\n",
    "1 2023-01-02  2023      1    January\n",
    "2 2023-01-03  2023      1    January\n",
    "3 2023-01-04  2023      1    January\n",
    "4 2023-01-05  2023      1    January\n",
    "~~~\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2c: Add Business-Useful Attributes**\n",
    "\n",
    "Let's add more attributes that analysts commonly need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3797ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add quarter, day_of_week, and is_weekend\n",
    "# Hint: EXTRACT(QUARTER ...), EXTRACT(DAYOFWEEK ...) where 0=Sunday, 6=Saturday\n",
    "# Hint: Use CASE WHEN for is_weekend\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE dim_date AS\n",
    "    SELECT\n",
    "        CAST(STRFTIME(full_date, '%Y%m%d') AS INTEGER) AS date_key,  -- e.g., 20240315\n",
    "        full_date,\n",
    "        EXTRACT(YEAR FROM full_date) AS year,\n",
    "        EXTRACT(MONTH FROM full_date) AS month,\n",
    "        STRFTIME(full_date, '%B') AS month_name,\n",
    "        EXTRACT(QUARTER FROM full_date) AS quarter,  -- 1, 2, 3, or 4\n",
    "        EXTRACT(DAYOFWEEK FROM full_date) AS day_of_week,  -- 0=Sun, 1=Mon, ..., 6=Sat\n",
    "        CASE\n",
    "            WHEN EXTRACT(DAYOFWEEK FROM full_date) IN (0, 6) THEN TRUE\n",
    "            ELSE FALSE\n",
    "        END AS is_weekend\n",
    "    FROM dim_date_basic\n",
    "\"\"\")\n",
    "\n",
    "print(\"Sample dim_date rows:\")\n",
    "print(conn.execute(\"SELECT * FROM dim_date LIMIT 10\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd734a67",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "Sample dim_date rows:\n",
    "   date_key   full_date  year  month month_name  quarter  day_of_week  is_weekend\n",
    "0  20230101  2023-01-01  2023      1    January        1            0        True  (Sunday)\n",
    "1  20230102  2023-01-02  2023      1    January        1            1       False  (Monday)\n",
    "2  20230103  2023-01-03  2023      1    January        1            2       False\n",
    "3  20230104  2023-01-04  2023      1    January        1            3       False\n",
    "4  20230105  2023-01-05  2023      1    January        1            4       False\n",
    "5  20230106  2023-01-06  2023      1    January        1            5       False\n",
    "6  20230107  2023-01-07  2023      1    January        1            6        True  (Saturday)\n",
    "...\n",
    "~~~\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2d: Link Fact Table to dim_date**\n",
    "\n",
    "Now we need to add a `date_key` column to `fact_sales` and populate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f55f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add the column\n",
    "conn.execute(\"\"\"\n",
    "    ALTER TABLE fact_sales ADD COLUMN date_key INTEGER;\n",
    "\"\"\")\n",
    "\n",
    "# Step 2: Populate it by converting order_date to YYYYMMDD format\n",
    "# TODO: Update fact_sales to set date_key\n",
    "# Hint: Use the same STRFTIME format as in dim_date\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "    UPDATE fact_sales\n",
    "    SET date_key = CAST(STRFTIME(order_date, '%Y%m%d') AS INTEGER);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Updated fact_sales with date_key:\")\n",
    "print(conn.execute(\"SELECT sale_id, order_date, date_key FROM fact_sales LIMIT 5\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ab939",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "Updated fact_sales with date_key:\n",
    "   sale_id  order_date  date_key\n",
    "0        1  2023-05-12  20230512\n",
    "1        2  2023-05-12  20230512\n",
    "2        3  2023-05-12  20230512\n",
    "3        4  2024-01-08  20240108\n",
    "4        5  2024-01-08  20240108\n",
    "~~~\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2e: Query with Rich Date Attributes**\n",
    "\n",
    "Now we can use all the pre-computed date attributes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Revenue by quarter\n",
    "print(\"Revenue by quarter:\")\n",
    "print(conn.execute(\"\"\"\n",
    "    SELECT\n",
    "        dd.year,\n",
    "        dd.quarter,\n",
    "        SUM(fs.line_total) as revenue\n",
    "    FROM fact_sales fs\n",
    "    JOIN dim_date dd ON fs.date_key = dd.date_key\n",
    "    GROUP BY dd.year, dd.quarter\n",
    "    ORDER BY dd.year, dd.quarter\n",
    "\"\"\").df())\n",
    "\n",
    "# Example 2: Weekend vs. weekday sales\n",
    "print(\"\\nWeekend vs. Weekday sales:\")\n",
    "print(conn.execute(\"\"\"\n",
    "    SELECT\n",
    "        CASE WHEN dd.is_weekend THEN 'Weekend' ELSE 'Weekday' END as period,\n",
    "        COUNT(*) as num_sales,\n",
    "        SUM(fs.line_total) as total_revenue\n",
    "    FROM fact_sales fs\n",
    "    JOIN dim_date dd ON fs.date_key = dd.date_key\n",
    "    GROUP BY dd.is_weekend\n",
    "\"\"\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5882789",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "Revenue by quarter:\n",
    "   year  quarter      revenue\n",
    "0  2023        1  XXXXXXXX.XX\n",
    "1  2023        2  XXXXXXXX.XX\n",
    "2  2023        3  XXXXXXXX.XX\n",
    "3  2023        4  XXXXXXXX.XX\n",
    "4  2024        1  XXXXXXXX.XX\n",
    "5  2024        2  XXXXXXXX.XX\n",
    "6  2024        3  XXXXXXXX.XX\n",
    "7  2024        4  XXXXXXXX.XX\n",
    "\n",
    "Weekend vs. Weekday sales:\n",
    "    period  num_sales  total_revenue\n",
    "0  Weekday      ~XXXX    XXXXXXXX.XX\n",
    "1  Weekend      ~XXXX    XXXXXXXX.XX\n",
    "~~~\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "#### **ðŸŽ¯ Key Takeaway**\n",
    "\n",
    "With `dim_date`, you now have:\n",
    "- âœ… Pre-computed date attributes (no `EXTRACT()` in every query)\n",
    "- âœ… Business-friendly names (`month_name` instead of numbers)\n",
    "- âœ… Complex date logic (weekends, quarters) in simple WHERE clauses\n",
    "- âœ… Consistent date handling across all fact tables\n",
    "\n",
    "**Industry Standard:** Every data warehouse has a `dim_date` table. You've just built one!\n",
    "\n",
    "---\n",
    "\n",
    "#### **Optional Challenge: Add Fiscal Year**\n",
    "\n",
    "Many companies have fiscal years that don't match calendar years (e.g., fiscal year starts April 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a706945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a fiscal_year column where fiscal year starts on July 1\n",
    "# Hint: If month >= 7, fiscal_year = calendar_year + 1, else fiscal_year = calendar_year\n",
    "\n",
    "# Example solution:\n",
    "conn.execute(\"\"\"\n",
    "    ALTER TABLE dim_date ADD COLUMN fiscal_year INTEGER;\n",
    "\"\"\")\n",
    "conn.execute(\"\"\"\n",
    "    UPDATE dim_date\n",
    "    SET fiscal_year = CASE\n",
    "        WHEN month >= 7 THEN year + 1\n",
    "        ELSE year\n",
    "    END;\n",
    "\"\"\")\n",
    "\n",
    "print(conn.execute(\"\"\"\n",
    "    SELECT full_date, year, month, fiscal_year\n",
    "    FROM dim_date\n",
    "    WHERE month IN (6, 7)\n",
    "    LIMIT 6\n",
    "\"\"\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf1648",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "~~~text\n",
    "   full_date  year  month  fiscal_year\n",
    "0 2023-06-29  2023      6         2023  (still FY 2023)\n",
    "1 2023-06-30  2023      6         2023\n",
    "2 2023-07-01  2023      7         2024  (FY 2024 starts!)\n",
    "3 2023-07-02  2023      7         2024\n",
    "4 2024-06-29  2024      6         2024\n",
    "5 2024-06-30  2024      6         2024\n",
    "~~~\n",
    "\n",
    "Shows the fiscal year transition on July 1.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Exercise 3: When to Normalize vs. Denormalize\n",
    "**Task:** For each scenario, recommend normalized or denormalized design and explain why.\n",
    "\n",
    "| Scenario | Your Recommendation | Why? |\n",
    "| :--- | :--- | :--- |\n",
    "| Banking transaction system | | |\n",
    "| Marketing dashboard | | |\n",
    "| IoT sensor data for real-time monitoring | | |\n",
    "| ML feature store for model training | | |\n",
    "| User profile management | | |\n",
    "\n",
    "<details>\n",
    "<summary>Expected Answers</summary>\n",
    "\n",
    "| Scenario | Recommendation | Why |\n",
    "| :--- | :--- | :--- |\n",
    "| Banking transaction system | **Normalized** | Data integrity critical, ACID compliance, frequent writes |\n",
    "| Marketing dashboard | **Denormalized (Star)** | Read-heavy, aggregations, historical snapshots |\n",
    "| IoT sensor data for real-time monitoring | **Denormalized (Time-series)** | Append-only writes (no updates), time-range queries, often uses specialized time-series DBs (InfluxDB, TimescaleDB) |\n",
    "| ML feature store for model training | **Denormalized** | Pre-computed features, read-heavy during training |\n",
    "| User profile management | **Normalized** | Frequent updates, data consistency important |\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Summary\n",
    "You have successfully:\n",
    "1.  Built both **normalized** and **star schema** versions of the same data.\n",
    "2.  Compared **query complexity** and **performance**.\n",
    "3.  Analyzed **storage** and **update** trade-offs.\n",
    "4.  Practiced designing **dimension tables** for analytical workloads.\n",
    "\n",
    "**Key Takeaway:** Neither approach is universally better. Match your schema design to your workload:\n",
    "*   **OLTP (transactional):** Normalize for integrity\n",
    "*   **OLAP (analytical):** Denormalize for speed"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
